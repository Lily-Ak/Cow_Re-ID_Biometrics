# -*- coding: utf-8 -*-
"""VGG16_CowID.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UkzZXk2lq2fLiANWxtGUP4vh7EIp00Uo
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR
import numpy as np

train_dir = '/content/drive/MyDrive/Colab Notebooks/Biometrics_Projects/BetterData/train'
validation_dir = '/content/drive/MyDrive/Colab Notebooks/Biometrics_Projects/BetterData/val'

# Data augmentation
train_transforms = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(30),
    transforms.ToTensor(),
])

validation_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])

# Load Data
train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

validation_dataset = datasets.ImageFolder(validation_dir, transform=validation_transforms)
validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)

# importing the VGG16 as pretrained
model = models.vgg16(pretrained=True)

# Freeze the initial layers
for param in model.features.parameters():
    param.requires_grad = False

# Modify the classifier to match the number of classes (6 in this case)
num_features = model.classifier[6].in_features
model.classifier[6] = nn.Linear(num_features, 6)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Creating the loss function and optimizer
loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)
scheduler = StepLR(optimizer, step_size=7, gamma=0.1)

# Early stopping parameters
patience = 10
best_val_loss = np.inf
patience_counter = 0

# Training loop with early stopping
num_epochs = 50
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    running_corrects = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        _, preds = torch.max(outputs, 1)
        running_corrects += torch.sum(preds == labels.data)

    epoch_loss = running_loss / len(train_dataset)
    epoch_acc = running_corrects.double() / len(train_dataset)

    model.eval()
    val_loss = 0.0
    val_corrects = 0

    with torch.no_grad():
        for inputs, labels in validation_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = loss_func(outputs, labels)

            val_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)

    val_loss = val_loss / len(validation_dataset)
    val_acc = val_corrects.double() / len(validation_dataset)

    print(f'Epoch {epoch}/{num_epochs - 1}')
    print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

    scheduler.step()
    # saving the model version that had best loss for the validation to use it later
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth')
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping")
            break

# Load the best model
model.load_state_dict(torch.load('best_model.pth'))

#using the best parameters to evaluate and plot results
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

model.eval()

# predictions and true labels
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in validation_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Convert lists to numpy arrays
all_preds = np.array(all_preds)
all_labels = np.array(all_labels)

# Calculate precision, recall, and F1 score
precision = precision_score(all_labels, all_preds, average='weighted')
recall = recall_score(all_labels, all_preds, average='weighted')
f1 = f1_score(all_labels, all_preds, average='weighted')
accuracy = accuracy_score(all_labels, all_preds)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

# Plot confusion matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()